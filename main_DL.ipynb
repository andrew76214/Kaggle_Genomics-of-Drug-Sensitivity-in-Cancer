{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec  6 16:16:11 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.120                Driver Version: 550.120        CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060        Off |   00000000:01:00.0  On |                  N/A |\n",
      "| 53%   48C    P2             50W /  170W |     194MiB /  12288MiB |     11%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    142762      G   /usr/lib/xorg/Xorg                             21MiB |\n",
      "|    0   N/A  N/A    250486      C   .../miniconda3/envs/pytorch/bin/python        162MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/andrew-root/.cache/kagglehub/datasets/samiraalipour/genomics-of-drug-sensitivity-in-cancer-gdsc/versions/2\n",
      "Loading Done!\n",
      "Preprocess Done!\n",
      "Define Done!\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from DataLoader import DataLoader\n",
    "from model.MLP import MLP_Tuner\n",
    "from model.CNNTransformer import CNNTransformer_Tuner\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samiraalipour/genomics-of-drug-sensitivity-in-cancer-gdsc\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "dataloader = DataLoader(path + '/GDSC_DATASET.csv',\n",
    "                        path + '/Compounds-annotation.csv',\n",
    "                        path + '/GDSC2-dataset.csv',\n",
    "                        path + '/Cell_Lines_Details.xlsx')\n",
    "\n",
    "X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, input_dim = dataloader.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m8.0902\u001b[0m  1.8236\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m8.1320\u001b[0m  1.9274\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m8.0729\u001b[0m  2.0535\n",
      "      2        \u001b[36m8.0268\u001b[0m  1.7980\n",
      "      2        \u001b[36m8.0789\u001b[0m  1.8036\n",
      "      2        \u001b[36m8.0364\u001b[0m  1.8255\n",
      "      3        \u001b[36m8.0157\u001b[0m  1.8203\n",
      "      3        8.0877  1.8416\n",
      "      3        \u001b[36m8.0319\u001b[0m  1.8199\n",
      "      4        8.0188  1.8159\n",
      "      4        \u001b[36m8.0742\u001b[0m  1.8158\n",
      "      4        8.0353  1.8167\n",
      "      5        \u001b[36m8.0154\u001b[0m  1.7995\n",
      "      5        8.0775  1.8386\n",
      "      5        \u001b[36m8.0286\u001b[0m  1.8305\n",
      "      6        \u001b[36m8.0153\u001b[0m  1.8668\n",
      "      6        \u001b[36m8.0695\u001b[0m  1.8545\n",
      "      6        8.0312  1.8245\n",
      "      7        8.0168  1.7975\n",
      "      7        8.0789  1.8286\n",
      "      7        \u001b[36m8.0281\u001b[0m  1.8240\n",
      "      8        8.0185  1.8469\n",
      "      8        8.0704  1.8440\n",
      "      8        \u001b[36m8.0225\u001b[0m  1.8203\n",
      "      9        \u001b[36m8.0128\u001b[0m  1.8274\n",
      "      9        8.0717  1.8011\n",
      "      9        8.0281  1.8059\n",
      "     10        8.0172  1.8383\n",
      "     10        \u001b[36m8.0691\u001b[0m  1.8452\n",
      "     10        8.0288  1.8362\n",
      "     11        8.0151  1.8509\n",
      "     11        8.0703  1.8405\n",
      "     11        8.0253  1.8185\n",
      "     12        8.0145  1.8242\n",
      "     12        8.0712  1.8255\n",
      "     12        8.0250  1.8283\n",
      "     13        8.0198  1.8451\n",
      "     13        8.0713  1.8306\n",
      "     13        8.0237  1.8024\n",
      "     14        8.0161  1.8205\n",
      "     14        \u001b[36m8.0681\u001b[0m  1.8540\n",
      "     14        8.0260  1.8252\n",
      "     15        \u001b[36m8.0127\u001b[0m  1.8438\n",
      "     15        \u001b[36m8.0678\u001b[0m  1.8478\n",
      "     15        \u001b[36m8.0212\u001b[0m  1.8235\n",
      "     16        8.0130  1.8055\n",
      "     16        \u001b[36m8.0664\u001b[0m  1.7915\n",
      "     16        8.0233  1.8150\n",
      "     17        \u001b[36m8.0124\u001b[0m  1.8132\n",
      "     17        8.0670  1.7975\n",
      "     17        8.0237  1.8121\n",
      "     18        \u001b[36m8.0109\u001b[0m  1.8006\n",
      "     18        8.0668  1.7962\n",
      "     18        8.0220  1.7978\n",
      "     19        8.0119  1.8031\n",
      "     19        8.0690  1.8311\n",
      "     19        8.0258  1.8282\n",
      "     20        8.0130  1.8375\n",
      "     20        \u001b[36m8.0649\u001b[0m  1.8109\n",
      "     20        8.0236  1.7890\n",
      "     21        \u001b[36m8.0098\u001b[0m  1.7913\n",
      "     21        8.0655  1.8295\n",
      "     21        8.0218  1.7911\n",
      "     22        8.0107  1.7795\n",
      "     22        8.0651  1.7836\n",
      "     22        8.0245  1.8860\n",
      "     23        8.0111  1.8393\n",
      "     23        8.0664  1.8436\n",
      "     23        8.0240  1.8232\n",
      "     24        8.0145  1.8108\n",
      "     24        8.0666  1.8484\n",
      "     24        \u001b[36m8.0210\u001b[0m  1.7832\n",
      "     25        8.0109  1.8289\n",
      "     25        8.0671  1.8736\n",
      "     25        8.0214  1.8227\n",
      "     26        8.0120  1.8329\n",
      "     26        8.0670  1.8689\n",
      "     26        8.0249  1.8204\n",
      "     27        8.0139  1.8133\n",
      "     27        8.0677  1.8474\n",
      "     27        \u001b[36m8.0209\u001b[0m  1.8160\n",
      "     28        \u001b[36m8.0086\u001b[0m  1.7986\n",
      "     28        8.0696  1.8719\n",
      "     28        \u001b[36m8.0202\u001b[0m  1.9361\n",
      "     29        8.0126  1.8694\n",
      "     29        8.0657  1.9245\n",
      "     29        8.0220  1.8370\n",
      "     30        8.0097  1.8386\n",
      "     30        8.0671  1.6998\n",
      "     30        8.0245  1.5966\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m8.0895\u001b[0m  3.5000\n",
      "      2        \u001b[36m8.0407\u001b[0m  3.1737\n",
      "      3        8.0453  3.2904\n",
      "      4        \u001b[36m8.0381\u001b[0m  3.2679\n",
      "      5        \u001b[36m8.0362\u001b[0m  3.2757\n",
      "      6        8.0374  3.3187\n",
      "      7        \u001b[36m8.0360\u001b[0m  3.2025\n",
      "      8        8.0373  3.2754\n",
      "      9        \u001b[36m8.0325\u001b[0m  3.2720\n",
      "     10        8.0364  3.2752\n",
      "     11        8.0334  3.2846\n",
      "     12        8.0342  3.2527\n",
      "     13        8.0347  3.3210\n",
      "     14        8.0339  3.2884\n",
      "     15        8.0356  3.2862\n",
      "     16        \u001b[36m8.0322\u001b[0m  3.3058\n",
      "     17        8.0335  3.1428\n",
      "     18        \u001b[36m8.0320\u001b[0m  3.2436\n",
      "     19        8.0324  3.2461\n",
      "     20        8.0336  3.2452\n",
      "     21        \u001b[36m8.0309\u001b[0m  3.1687\n",
      "     22        8.0350  3.2609\n",
      "     23        8.0375  3.3185\n",
      "     24        8.0330  3.2564\n",
      "     25        8.0343  3.3287\n",
      "     26        8.0335  3.3355\n",
      "     27        \u001b[36m8.0304\u001b[0m  3.3767\n",
      "     28        8.0312  3.3350\n",
      "     29        8.0337  3.2430\n",
      "     30        8.0308  3.3387\n",
      "Best Parameters: {'lr': 0.01, 'module__cnn_filters': 32, 'module__nhead': 2, 'module__num_layers': 1, 'module__transformer_dim': 64, 'optimizer__weight_decay': 0.001}\n",
      "Best Score: -8.028114477793375\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tuner\n",
    "CNNTransformer_tuner = CNNTransformer_Tuner(input_dim)\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_model = CNNTransformer_tuner.tune_hyperparameters(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     31        8.0320  3.3407\n",
      "     32        8.0333  3.3361\n",
      "     33        8.0327  3.3384\n",
      "     34        8.0315  3.2540\n",
      "     35        8.0324  3.3078\n",
      "     36        8.0317  3.3232\n",
      "     37        8.0320  3.3226\n",
      "     38        \u001b[36m8.0301\u001b[0m  3.3075\n",
      "     39        8.0334  3.2392\n",
      "     40        8.0318  3.3031\n",
      "     41        8.0322  3.3073\n",
      "     42        8.0332  3.2930\n",
      "     43        8.0314  3.3637\n",
      "     44        8.0404  3.3733\n",
      "     45        8.0312  3.3510\n",
      "     46        8.0314  3.2920\n",
      "     47        8.0328  3.1518\n",
      "     48        8.0357  3.3066\n",
      "     49        8.0311  3.2277\n",
      "     50        8.0323  3.3452\n",
      "     51        8.0321  3.2586\n",
      "     52        8.0312  3.2149\n",
      "     53        8.0315  3.3725\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1): # best_model.max_epochs\n",
    "    best_model.partial_fit(X_train_tensor, y_train_tensor)\n",
    "    train_pred = best_model.predict(X_train_tensor).squeeze()\n",
    "    val_pred = best_model.predict(X_test_tensor).squeeze()\n",
    "    train_loss = mean_squared_error(y_train_tensor.numpy(), train_pred)\n",
    "    val_loss = mean_squared_error(y_test_tensor.numpy(), val_pred)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Plot training and validation loss to check for overfitting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, best_model.max_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, best_model.max_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "# best_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = best_model.predict(X_test_tensor).squeeze()\n",
    "    predictions = torch.tensor(predictions)    \n",
    "    \n",
    "    # Calculate RMSE, MAE, and MSE\n",
    "    rmse = torch.sqrt(nn.MSELoss()(predictions, y_test_tensor)).item()\n",
    "    mae = mean_absolute_error(y_test_tensor.numpy(), predictions.numpy())\n",
    "    mse = mean_squared_error(y_test_tensor.numpy(), predictions.numpy())\n",
    "    \n",
    "    print(f\"Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"Test MAE: {mae:.4f}\")\n",
    "    print(f\"Test MSE: {mse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
